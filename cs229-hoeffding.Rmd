---
output:
  html_document:
    toc: TRUE
    fig_width: 4.5
    css: /home/ebosi/github/james-chuang.github.io/_sass/_style.scss
---

My notes on John Duchi's [CS229 supplemental notes on Hoeffding's inequality](http://cs229.stanford.edu/materials.html).

## **basic probability bounds**

  - a basic question in probability, statistics, and machine learning:
      - given a random variable $Z$ with expectation $\mathbb{E}[Z]$, how likely is $Z$ to be close to its expectation?
      - more precisely, how close is it likely to be?
      - therefore, we would like to compute bounds of the following form for $t \geq 0$
      
      $$
      P(Z \geq \mathbb{E}[Z] + t) \text{  and  } P(Z \leq \mathbb{E}[Z]-t)
      $$
      
  - **Markov's inequality**
      - Let $Z \geq 0$ be a non-negative random variable. Then for all $t \geq 0$,
      
      $$
      P(Z \geq t) \leq \frac{\mathbb{E}[Z]}{t}
      $$
      
      - i.e., Markov's inequality puts a bound on the probability that a random variable is greater than a non-negative value $t$
      - Proof:
          - note: $P(Z \geq t) = \mathbb{E} \left[\mathbf{1} \left\{Z \geq t \right\} \right]$
              - consider the two possible cases for $Z$:
                  - if $Z \geq t$, then $\mathbf{1}\{Z \geq t \} = 1$:
                  
                  $$
                  \begin{align}
                  Z & \geq t \\
                  \frac{Z}{t} & \geq 1 \\
                  \frac{Z}{t} & \geq \mathbf{1}\{Z \geq t \}
                  \end{align}
                  $$
                  
                  - if $Z < t$, then $\mathbf{1}\{Z \geq t \} = 0$:
                  
                  $$
                  \begin{align}
                  \frac{Z}{t} & \geq 0 && \text{Z and t both } > 0 \\
                  \frac{Z}{t} & \geq \mathbf{1}\{Z \geq t\}
                  \end{align}
                  $$
                  
                  - so in general, $\frac{Z}{t} \geq \mathbf{1}\{Z \geq t\}$
          - thus:
          
          $$
          \begin{align}
          P(Z \geq t) & = \mathbb{E}\left[\mathbf{1} \left\{Z \geq t \right\} \right] \\
          P(Z \geq t) & \leq \mathbb{E} \left[\frac{Z}{t} \right] \\
          P(Z \geq t) & \leq \frac{\mathbb{E}[Z]}{t} 
          \end{align}
          $$
          
      - **note:** this is the proof given in the notes, but [this proof](http://mathworld.wolfram.com/MarkovsInequality.html) from Wolfram Alpha makes more sense to me
  - essentially all other bounds on probabilities are variations on Markov's inequality
      - the first variation uses second moments -- the variance -- of a random variable rather than simply its mean, and is known as Chebyshev's inequality
  - **Chebyshev's inequality**
      - Let $Z$ be any random variable with $\text{Var}(Z) < \infty$. Then, for $t \geq 0$,
      
      $$
      P \left(\left( Z - \mathbb{E}[Z]\right)^2 \geq t^2 \right) \leq \frac{\text{Var}(Z)}{t^2}
      $$
      
      - i.e., Chebyshev's inequality puts a bound on the probability that a random variable is greater than $t$ away from its expected value $\mathbb{E}[Z]$
      - Proof:
          
          $$
          \begin{align}
          & \quad  P \left( \left(Z - \mathbb{E}[Z] \right)^2 \geq t^2 \right) \\
          & \leq \frac{\mathbb{E}[\left(Z - \mathbb{E}[Z]\right)^2]}{t^2} && \text{by Markov's inequality} \\
          & \leq \frac{\text{Var}(Z)}{t^2}
          \end{align}
          $$
          
      - a nice consequence of Chebyshev's inequality:
          - averages of random variables with finite variance converge to their mean
          - an example:
              - suppose $Z_i$ are i.i.d. and satisfy $\mathbb{E} \left[Z_i \right]=0$
              - define $\bar{Z} = \frac{1}{n} \sum_{i=1}^n Z_i$
              - then:
              
              $$
              \begin{align}
              & \quad \text{Var}(\bar{Z}) \\
              & = \mathbb{E} \left[\bar{Z}^2 \right] - \mathbb{E}^2 \left[\bar{Z} \right] && \text{def. variance}\\
              & = \mathbb{E} \left[\left(\frac{1}{n}  \sum_{i=1}^n Z_i \right)^2 \right]  && \mathbb{E}\left[\bar{Z} \right] = 0, \text{ since } \mathbb{E}\left[Z_i \right] = 0 \text{ and expectations are linear}\\
              & = \frac{1}{n^2} \sum_{i,j \leq n} \mathbb{E} \left[Z_i Z_j \right] \\
              & = \frac{1}{n^2} \sum_{i=1}^n \mathbb{E} \left[Z_i^2 \right] \\
              & = \frac{\text{Var} \left(Z_1 \right)}{n}
              \end{align}
              $$
              
              - in particular, for any $t \geq 0$:
              
              $$
              P \left(\left\lvert \frac{1}{n} \sum_{i=1}^n Z_i \right\rvert \geq t \right) \leq \frac{\text{Var}\left(Z_1 \right)}{nt^2}
              $$
              
              - so, $P \left(\left\lvert \bar{Z} \right\rvert \geq t \right) \rightarrow 0$ for any $t>0$
              
## **moment generating functions**

  - often, we want sharper -- even exponential -- bounds on the probability that a random variable exceeds its expectation by much
      - to accomplish this, we need a stronger condition than finite variance
      - **moment generating functions** are natural candidates for this condition:
          - for a random variable $Z$, the moment generating function of $Z$ is the function:
          
          $$
          M_Z (\lambda) := \mathbb{E} \left[\exp (\lambda Z) \right]
          $$
          
          - the moment generating function may be infinite for some $\lambda$
          
### **Chernoff bounds**

  - **Chernoff bounds** use moment generating functions to give exponential deviation bounds
      - Let $Z$ be any random variable
      - then, for any $t \geq 0$
      
      $$
      \begin{align}
      P \left(Z \geq \mathbb{E}[Z] + t \right) & \leq \min_{\lambda \geq 0} \mathbb{E} \left[e^{\lambda \left(Z - \mathbb{E}[Z] \right)} \right] e^{-\lambda t} \\
       & \leq \min_{\lambda \geq 0} M_{Z - \mathbb{E}[Z]} \left(\lambda \right) e^{-\lambda t} \\
       \\
       \text{and} \\
       P \left(Z \leq \mathbb{E}[Z] - t \right) & \leq \min_{\lambda \geq 0} \mathbb{E} \left[e^{\lambda \left(\mathbb{E}[Z]-Z \right)} \right] e^{-\lambda t} \\
       & \leq \min_{\lambda \geq 0} M_{\mathbb{E}[Z]-Z} \left(\lambda \right) e^{-\lambda t}
      \end{align}
      $$
      
      - proof of the first inequality (the second inequality is identical):
          - for any $\lambda > 0$:
              - $Z \geq \mathbb{E}[Z]+t$ iff $e^{\lambda Z} \geq e^{\lambda \mathbb{E}[Z]+ \lambda t}$, i.e. $e^{lambda \left(Z- \mathbb{E}[Z] \right)}$
              - thus,
              
              $$
              \begin{align}
              P\left(Z - \mathbb{E}[Z] \geq t \right) & = P\left(e^{\lambda \left(Z - \mathbb{E}[Z] \right)} \geq e^{\lambda t}\right) \\
              & \leq \mathbb{E} \left[e^{\lambda \left(Z - \mathbb{E}[Z] \right)} \right] e^{- \lambda t} && \text{by Markov's inequality}
              \end{align}
              $$