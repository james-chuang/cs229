---
output:
  html_document:
    toc: TRUE
    fig_width: 4.5
    css: /home/ebosi/github/james-chuang.github.io/_sass/_style.scss
---

My notes on Andrew Ng's [CS229 lecture 2 notes](http://cs229.stanford.edu/materials.html).

# **Generative Learning algorithms**

***discriminative*** learning algorithms

  - learn $p(y \mid x)$ directly
      - e.g. logistic regression
  - or, learn mapping from the space of inputs, $\mathcal{X}$ to the labels
      - e.g. perceptron algorithm
      
***generative*** learning algorithms

  - model $p(x \mid y)$ (and $p(y)$, the ***class priors***)
      - e.g. if 0 = dog and 1 = elephant, $p(x \mid y=0)$ models the distribution of dogs' features, and $p(x \mid y = 1)$ models the distribution of elephants' features
  - then use Bayes rule to derive the posterior distribution on $y$ given $x$:
  
    $$
    p(y \mid x) = \frac{p(x \mid y) p (y)}{p(x)}
    $$
    
      - to use $p(y \mid x)$ to make a prediction, the denominator $p(x)$ does not need to be calculated:
      
      $$
      \begin{align}
      \arg \max_y p(y \mid x) & = \arg \max_y \frac{p(x \mid y) p(y)}{p(x)} \\
                              & = \arg \max_y p(x \mid y)p(y)
      \end{align}
      $$

## **Gaussian discriminant analysis**

Assume that $p(x \mid y)$ is distributed according to a multivariable normal distribution.

### ***The multivariate normal distribution***

parameters

  - ***mean vector*** $\mu \in \mathbb{R}^n$
  - ***covariance matrix*** $\Sigma \in \mathbb{R}^{n \times n}$
    - $\Sigma \geq 0$ is symmetric and positive semi-definite
    
  $$
  p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}} \lvert \Sigma \rvert^{\frac{1}{2}}} \exp \left(-\frac{1}{2} \left(x-\mu \right)^T \Sigma^{-1} \left(x-\mu \right) \right)
  $$
For $X \sim \mathcal{N}(\mu, \Sigma)$:

$$
\begin{align}
\text{E}[X] & = \int_x x p(x;\mu,\Sigma) dx = \mu
\end{align}
$$
For a vector-valued random variable $Z$, $\text{Cov}(Z) = \text{E} \left[(Z-\text{E}[Z]) (Z-\text{E}[Z])^T \right]$, generalizing the variance to multiple dimensions.

$$
\begin{align}
\text{Cov}(Z) & = \text{E} \left[(Z-\text{E}[Z]) (Z-\text{E}[Z])^T \right] \\
              & = \text{E} \left[(Z-\text{E}[Z]) \left(Z^T-\left(\text{E}[Z]\right)^T\right) \right] & \text{transpose respects addition}                                   \\
              & = \text{E} \left[\left(Z-\text{E}[Z]\right) \left(Z^T-\text{E}[Z] \right) \right] & \text{transpose of a scalar: }\text{E}[Z] = \left(\text{E}[Z]\right)^T \\
              & = \text{E} \left[ZZ^T- Z \text{E}[Z] - \text{E}[Z]Z^T + \text{E}^2[Z] \right] \\
              & = \text{E} \left[ZZ^T \right] - \text{E}[Z\text{E}[Z]] - \text{E}[\text{E}[Z]Z^T] + \text{E}[\text{E}^2[Z]] \\
              & = \text{E} \left[ZZ^T \right] - \text{E}^2[Z] - \text{E}[Z]\text{E}\left[Z^T\right] + \text{E}^2[Z] & \text{expectation of a scalar: }\text{E}[\text{E}[Z]] = \text{E}[Z] \\
              & = \text{E} \left[ZZ^T \right] - \text{E}[Z]\text{E}\left[Z^T \right] \\
              & = \text{E} \left[ZZ^T \right] - \left(\text{E}[Z]\right)\left(\text{E}\left[Z \right]\right)^T \\
              \\
          \therefore  \text{Cov}(Z) & = \text{E} \left[(Z-\text{E}[Z]) (Z-\text{E}[Z])^T \right] \\
                                    & = \text{E} \left[ZZ^T \right] - \left(\text{E}[Z]\right)\left(\text{E}\left[Z \right]\right)^T
\end{align}
$$
If $X \sim \mathcal{N}(\mu, \Sigma)$, then
$$
\text{Cov}(X) = \Sigma
$$
  
$X \sim \mathcal{N}(\mu = \mathbf{0}, \Sigma = I)$ is the ***standard normal distribution***.
  
The diagonal of the covariance matrix is the variance of each variable. The off-diagonal elements $\Sigma_{ij}, i \neq j$ are the covariance of variable $i$ with variable $j$.

## **The Gaussian Discriminant Analysis model**
Classification with continuous-valued input features $x$, modelling $p(x \mid y)$ using a multivariate normal distribution:

$$
\begin{align}
y             & \sim \text{Bernoulli}(\phi)   \\
x \mid y = 0  & \sim \mathcal{N}(\mu_0, \Sigma)  \\
x \mid y = 1  & \sim \mathcal{N}(\mu_1, \Sigma)
\end{align}
$$
, i.e.
$$
\begin{align}
p(y)            & = \phi^y (1-\phi)^{1-y} \\
p(x \mid y = 0) & = \frac{1}{(2\pi)^{\frac{n}{2}} \lvert \Sigma \rvert^{\frac{1}{2}}} \exp \left(- \frac{1}{2} (x-\mu_0)^T \Sigma^{-1}(x-\mu_0) \right) \\
p(x \mid y = 1) & = \frac{1}{(2\pi)^{\frac{n}{2}} \lvert \Sigma \rvert^{\frac{1}{2}}} \exp \left(- \frac{1}{2} (x-\mu_1)^T \Sigma^{-1}(x-\mu_1) \right) \\
\end{align}
$$
Parameters of the model: $\phi, \Sigma, \mu_0, \mu_1$. Note the assumption of equal covariance matrices between the classes. Relaxing this assumption leads to quadratic discriminant analysis (QDA).

The log-likelihood of the data:
$$
\begin{align}
\ell(\phi, \mu_0, \mu_1, \Sigma) & = \log \prod_{i=1}^m p \left(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma \right) \\
                                & = \log \prod_{i=1}^m p \left(x^{(i)} \mid y^{(i)}; \mu_0, \mu_1, \Sigma \right)p\left(y^{(i)}; \phi\right) & \text{chain rule of conditional prob.}\\
\end{align}
$$

## **GDA and logistic regression**
The GDA model can be thought of as the generative analog to the discriminative logistic regression algorithm:
$$
\begin{align}
p(y=1 \mid x; \phi, \mu_0, \mu_1, \Sigma) & = \frac{1}{(2\pi)^{\frac{n}{2}} \lvert \Sigma \rvert^{\frac{1}{2}}} \exp \left(- \frac{1}{2} (x-\mu_1)^T \Sigma^{-1}(x-\mu_1) \right) \\
                                          & = \frac{1}{1+ \exp(-\theta^Tx)}, \qquad\text{where } \theta = f\left(\phi, \Sigma, \mu_0, \mu_1 \right)
\end{align}
$$

, which is the form of logistic regression. The above argument says that if $p(x \mid y)$ is multivariate gaussian (with shared $\Sigma$), then $p(y \mid x)$ follows a logistic function. However, the converse is not true: $p(y \mid x)$ being a logistic function does not imply $p(x \mid y)$ is multivariate Gaussian. So, GDA makes *stronger* modeling assumptions about the data than logistic regression. When these assumptions are correct, then GDA will better fit to the data, and is a better model. Specifically, when $p(x \mid y)$ is Gaussian (with shared $\Sigma$), then GDA is ***asymptotically efficient***- in the limit of large training sets, there is no algorithm that is strictly better than GDA. In constrast, logistic regression makes weaker assumptions about the data and is thus more *robust* to incorrect modelling assumptions. When the data is non-Gaussian, then in the limit of large datasets, logistic regression will usually outperform GDA.

## **Naive Bayes**
- Consider $x_i$'s which are discrete-valued, e.g. vectors where each element represents the $i$-th word of a dictionary.
  - $x_i = 1$ if the word is included, otherwise $x_i =0$.
  - set of words encoded in the feature vector $\rightarrow$ **vocabulary**.
    - dimension of $x$ = size of vocabulary
  - to build a generative model, have to model $p(x \mid y)$
    - consider vocabulary of 50,000 words
      - then, $x \in \{0,1\}^{50000}$
        - modelling $x$ with a multinomial distribution over the $2^{50000}$ possible outcomes results in a $(2^{50000}-1)$-dimensional parameter vector (way too many.).
        - to make $p(x \mid y)$ tractable, make a strong assumption- the ***Naive Bayes (NB) assumption***, which assumes that the $x_i$'s are conditionally independent given $y$. The resulting algorithm is called the ***Naive Bayes classifier***. Then,
        
        $$
        \begin{align}
        p(& x_1, \dots, x_{50000} \mid y) \\
        & = p(x_1 \mid y)p(x_2 \mid y, x_1)p(x_3 \mid y, x_1, x_2) \cdots p(x_{50000} \mid y, x_1, \dots, x_{49999}) & \text{chain rule of probabilities} \\
        & = p(x_1 \mid y)p(x_2 \mid y)p(x_3 \mid y) \cdots p(x_{50000} \mid y) & \text{NB assumption} \\
        & = \prod_{i=1}^n p(x_i \mid y)
        \end{align}
        $$
        
        - the Naive Bayes assumption is an extremely strong assumption, but the resulting algorithm works well on many problems
- model parameters:
  $$
  \begin{align}
  \phi_{i | y = 1}  & = p(x_i = 1 \mid y=1), \\
  \phi_{i | y = 0}  & = p(x_i = 1 \mid y=0), \\
  \phi_{y}          & = p(y=1)
  \end{align}
  $$
  
- joint likelihood:
$$
\mathcal{L}(\phi_{y}, \phi_{i | y = 0}, \phi_{i | y = 1}) = \prod_{i=1}^m p\left(x^{(i)}, y^{(i)} \right)
$$

Maximize joint likelihood w.r.t. $\phi_y$, $\phi_{i \mid y=0}$ and $\phi_{i \mid y=1}$ to get maximum likelihood estimates:
$$
\begin{align}
\phi_{j \mid y=1} & = \frac{\sum_{i=1}^m 1 \left\{x_j^{(i)}=1 \hspace{.2em} \wedge \hspace{.2em} y^{(i)} = 1 \right\}}{\sum_{i=1}^m 1 \left\{y^{(i)}=1 \right\}} \\
\phi_{j \mid y=0} & = \frac{\sum_{i=1}^m 1 \left\{x_j^{(i)}=1 \hspace{.2em} \wedge \hspace{.2em} y^{(i)} = 0 \right\}}{\sum_{i=1}^m 1 \left\{y^{(i)}=0 \right\}} \\
\phi_y            & = \frac{\sum_{i=1}^m 1 \left\{y^{(i)}=1 \right\}}{m}
\end{align}
$$

Where the $\wedge$ symbol means "and". The parameters have very natural interpretations:

  - $\phi_{j \mid y =1}$: the fraction of examples of class 1 in which feature $j$ appears
  - $\phi_{j \mid y =0}$: the fraction of examples of class 0 in which feature $j$ appears
  - $\phi_y$: the fraction of examples which are of class 1
  
To make a prediction on a new example with features $x$, calculate:
$$
\begin{align}
p(y=1 \mid x) & = \frac{p(x \mid y=1) p(y=1)}{p(x)} \\
              & = \frac{\left(\prod_{i=1}^n  p(x_i \mid y =1) \right)p(y=1)}{ \left(\prod_{i=1}^n p(x_i \mid y = 1) \right)p(y=1) + \left(\prod_{i=1}^n p(x_i \mid y = 0) \right) p(y=0)}
\end{align}
$$

and classify to whichever class has the higher posterior probability.

Naive Bayes can be generalized to the case where $x_i \in \left\{1, 2, \dots, k \right\}$ by modelling $p(x_i \mid y)$ as multinomial instead of Bernoulli.

Continuous input variables can be adapted for Naive Bayes by ***discretizing***, i.e. splitting the continuous value into bins. When continuous input variables are not well-modeled by a multivariate normal distribution, discretizing and using Naive Bayes often results in a better classifier than GDA.

## **Laplace smoothing**
The Naive Bayes algorithm runs into a problem when it encounters examples with features that were not seen in the training set. For such a feature $f$, maximum likelihood picks the following parameters:
$$
\begin{align}
\phi_{f \mid y=1} & = 0 \\
\phi_{f \mid y=0} & = 0
\end{align}
$$

If the feature is present in an example to be classified, then the class posterior probability is as follows:
$$
\begin{align}
p(y=1 \mid x) & = \frac{\left(\prod_{i=1}^n  p(x_i \mid y =1) \right)p(y=1)}{ \left(\prod_{i=1}^n p(x_i \mid y = 1) \right)p(y=1) + \left(\prod_{i=1}^n p(x_i \mid y = 0) \right) p(y=0)} \\
              & = \frac{0}{0} \qquad \text{, since each } \prod_{i=1}^n p(x_i \mid y) \text{ contains } p(x_f \mid y) = 0 
\end{align}
$$

The problem is that the prior probability for features that are not seen in the finite training set is set to zero, when it is unlikely to be exactly zero. This can be addressed by ***Laplace smoothing***, which replaces the estimates for $\phi_j$ with 
$$
\phi_j = \frac{\sum_{i=1}^m 1 \left\{z^{(i)} = j \right\} + 1}{m + k} 
$$

The maximum likelihood estimates of the parameters for Naive Bayes with Laplace smoothing:
$$
\begin{align}
\phi_{j \mid y=1} & = \frac{\sum_{i=1}^m 1 \left\{x_j^{(i)}=1 \hspace{.2em} \wedge \hspace{.2em} y^{(i)} = 1 \right\} + 1}{\sum_{i=1}^m 1 \left\{y^{(i)}=1 \right\} +2} \\
\phi_{j \mid y=0} & = \frac{\sum_{i=1}^m 1 \left\{x_j^{(i)}=1 \hspace{.2em} \wedge \hspace{.2em} y^{(i)} = 0 \right\} + 1}{\sum_{i=1}^m 1 \left\{y^{(i)}=0 \right\} +2}
\end{align}
$$

## **Event models for text classification**
Naive Bayes as presented above uses the ***multi-variate Bernoulli event model***. This model assumes that the class priors first determine which class an example comes $(p(y))$. Then, the class-conditional probabilities $p(x_i=1 \mid y) = \phi_{i \mid y}$ are used to decide whether each word in the dictionary is added to the email, independently of the other words. The probability of a message is $p(y) \prod_{i=1}^n p(x_i \mid y)$.

A different model is the ***multinomial event model***.

  - Let $x_i$ denote the identity of the $i$-th word
    - $x_i \in  \left\{1, \dots, \lvert V \rvert \right\}$
      - $\lvert V \rvert$ is the size of the vocabulary (dictionary)
  - an email of $n$ words is represented by a vector $(x_1, x_2, \dots, x_n)$ of length $n$, where $n$ can vary for different documents
  
To generate a document. Use class priors to choose which class (same as Naive Bayes). Generate $x_1$ from a multinomial distribution over words $(p(x_1 \mid y))$, then generate $x_2$ independently but from the same distribution and so on until $x_n$. The probability of a message is $p(y) \prod_{i=1}^n p(x_i \mid y)$, which looks like the probability under Naive Bayes, but differs since $x_i \mid y$ is multinomial rather than Bernoulli.

Parameters for the multinomial event model:
$$
\begin{align}
\phi_y & = p(y) \\
\phi_{k \mid y=1} & = p(x_j = k \mid y=1) & \text{(for any j)} \\
\phi_{i \mid y=0} & = p(x_j = k \mid y=0)
\end{align}
$$