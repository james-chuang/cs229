---
output:
  html_document:
    toc: TRUE
    fig_width: 4.5
    css: /home/ebosi/github/james-chuang.github.io/_sass/_style.scss
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **CS229 Autumn 2016 Problem Set #1**

### **1. Logistic Regression**

**a.** Consider the average empirical loss (the risk) for logistic regression:
$$
\begin{align}
J(\theta) & = \frac{1}{m} \sum_{i=1}^m \log \left(1+e^{-y^{(i)}\theta^Tx^{(i)}} \right) \\
          & = - \frac{1}{m} \sum_{i=1}^m \log \left(h_\theta \left(y^{(i)} x^{(i)} \right) \right) \\
          & &h_\theta(x) & = g(\theta^Tx), \\
          & &g(z) & = \frac{1}{1+e^{-z}}
\end{align}
$$
Find the Hessian $H$ of this function, and show that for any vector $z$, it holds true that
$$
z^THz \geq 0.
$$
*Hint:* You might want to start by showing the fact that $\sum_i \sum_j z_i x_i x_j z_j = \left(x^Tz \right)^2 \geq 0$.

**Remark:** This is one of the standard ways of showing that the matrix $H$ is positive semidefinite, written "$H \succeq 0$." This implies that $J$ is convex, and has no local minima other than the global one. If you have some other way of showing $H \succeq 0$, you're also welcome to use your method instead of the one above.

$$
\begin{align}
J(\theta)                                     & = \frac{1}{m} \sum_{i=1}^m \log \left(1+e^{-y^{(i)}\theta^Tx^{(i)}} \right) \\
\frac{\partial J(\theta)}{\partial \theta_k}  & = \frac{1}{m} \sum_{i=1}^m \frac{e^{-y^{(i)}\theta^Tx^{(i)}}}{1+e^{-y^{(i)}\theta^Tx^{(i)}}} \left(-y^{(i)} x_k^{(i)} \right) \\
\frac{\partial J(\theta)}{\partial \theta_k}  & = \frac{1}{m} \sum_{i=1}^m \frac{1}{1 + e^{y^{(i)}\theta^Tx^{(i)}}} \left(-y^{(i)} x_k^{(i)} \right) \\
\frac{\partial J(\theta)}{\partial \theta_k}  & = - \frac{1}{m} \sum_{i=1}^m h_\theta \left(-y^{(i)}x^{(i)} \right) \left(y^{(i)} x_k^{(i)} \right) \\
\frac{\partial^2 J(\theta)}{\partial \theta_\ell \partial \theta_k} & = - \frac{1}{m} \sum_{i=1}^m \frac{\partial}{\partial \theta_\ell} h_\theta \left(-y^{(i)}x^{(i)} \right) \left(y^{(i)} x_k^{(i)} \right) \\
\frac{\partial^2 J(\theta)}{\partial \theta_\ell \partial \theta_k} & = - \frac{1}{m} \sum_{i=1}^m h_\theta \left(-y^{(i)} x^{(i)} \right) \left(1-h_\theta \left(-y^{(i)}x^{(i)} \right) \right) \left(-y^{(i)}x_k^{(i)} \right) \left(-y^{(i)}x_\ell^{(i)} \right) \\

& = -\frac{1}{m} \sum_{i=1}^m h_\theta \left(x^{(i)} \right) \left(1- h_\theta \left(x^{(i)} \right) \right) x_\ell^{(i)} x_k^{(i)}  ????????????????
\end{align}
$$

**b.** We have provided two data files:
  - http://cs229.stanford.edu/ps/ps1/logistic_x.txt
  - http://cs229.stanford.edu/ps/ps1/logistic_y.txt
  
These files contain the inputs $\left(x^{(i)} \in \mathbb{R}^2 \right)$ and outputs $\left(y^{(i)} \in \left\{-1,1 \right\} \right)$, respectively, for a binary classification problem, with one training example per row. Implement Newton's method for optimizing $J(\theta)$, and apply it to fit a logistic regression model to the data. Initialize Newton's method with $\theta = \vec{0}$ (the vector of all zeros). What are the coefficients $\theta$ resulting from your fit? (Remember to include the intercept term.)

```{python}
```

**c.** Plot the training data (your axes should be $x_1$ and $x_2$, corresponding to the two coordinates of the inputs, and you should use a different symbol for each point plotted to indicate whether that example has label $1$ or $-1$). Also plot on the same figure the decision boundary fit by logistic regression. (This should be a straight line showing the boundary separating the region where $h_\theta(x) > 0.5$ from where $h_\theta(x) \leq 0.5$.)


