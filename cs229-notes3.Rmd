---
output:
  html_document:
    toc: TRUE
    fig_width: 4.5
    css: /home/ebosi/github/james-chuang.github.io/_sass/_style.scss
---

My notes on Andrew Ng's [CS229 lecture 3 notes](http://cs229.stanford.edu/materials.html).

# **Support Vector Machines**

## **1. Margins: Intuition**

Consider logistic regression:

  - $p(y = 1 \mid x ; \theta)$ is modeled by $h_\theta(x) = g \left(\theta^Tx \right)$
    - $h_\theta(x) \geq 0.5$, i.e. $\theta^Tx \geq 0  \rightarrow$ predict "1"
    - larger $\theta^Tx$, larger $h_\theta(x) = p(y=1 \mid x; w,b)$, higher "confidence" in prediction of label 1
        - informally, a prediction $y=1$ is a very confident one if $\theta^Tx \gg 0$
        - similarly, a prediction $y=0$ is a very confident one if $\theta^Tx \ll 0$
            - therefore, a good fit to the data would be to find $\theta$ such that $\theta^Tx^{(i)} \gg 0$ whenever $y^{(i)}=1$, and $\theta^Tx^{(i)} \ll 0$ whenever $y^{(i)}= 0$.
    - geometrically, points far away from the ***separating hyperplane*** can be predicted with higher confidence than points close to the separating hyperplane
    
## **2. Notation**

Consider a linear classifier for a binary classification problem with labels $y$ and features $x$:

  - $y \in \left\{-1, 1 \right\}$
  - parameters $w, b$ (treat the bias/intercept $b$ separately from the weights $w$)
  - write classifier as:
  
  $$
  \begin{align}
  h_{w,b}(x) & = g(w^Tx + b) \\
  \\
  g(z) & =  \begin{cases}
          1 & \text{if } z \geq 0 \\
          -1 & \text{otherwise}
          \end{cases}
  \end{align}
  $$
  Note that from the definition of $g$, this classifier directly predicts either $1$ or $-1$ without first going through the intermediate step of estimating the probability of $y$ being $1$, as in logistic regression.
  
## **3. Functional and geometric margins**

##### **Functional margins**
Given a training example $\left(x^{(i)}, y^{(i)} \right)$, define the **functional margin** $\hat{\gamma}^{(i)}$ of $\left(w,b \right)$ w.r.t. the training example:

$$
\hat{\gamma}^{(i)} = y^{(i)} \left(w^Tx + b \right)
$$
Note:
$$
\begin{cases}
\text{if }y^{(i)} = 1, \text{ then }\hat{\gamma}^{(i)} \gg 0 \hspace{1em}\text{if} \hspace{1em} w^Tx+b \gg 0 \\
\text{if }y^{(i)} = -1, \text{ then } \hat{\gamma}^{(i)} \gg 0 \hspace{1em}\text{if} \hspace{1em} w^Tx+b \ll 0
\end{cases}
$$
A prediction is correct if $y^{(i)} \left(w^Tx+b \right) > 0$. Large functional margin = a confident and correct prediction.

One property of this classifier needs to be addressed: $g$, and hence $h_{w,b}(x)$ depends on the sign, but *not* on the magnitude of $w^Tx+b$. (E.g., $g(w^Tx+b) = g(2w^Tx+2b)$). Therefore, without an additional normalization condition, the functional margin can be made arbitrarily large by scaling $w$ and $b$. We will come back to the normalization condition later.

Given a training set $S = \left\{\left(x^{(i)}, y^{(i)}\right); i = 1, \dots, m \right\}$, define the ***functional margin*** $\hat{\gamma}$ of $(w,b)$ w.r.t. $S$ to be the smallest of the functional margins of the individual training examples:
$$
\hat{\gamma} = \min_{i=1, \dots, m} \hat{\gamma}^{(i)}
$$
The functional margin simply tells you whether a particular point is properly classified or not. In order to be able to maximize the margin, there needs to be a notion of magnitude. Therefore, we introduce the ***geometric margin***, a scaled version of the functional margin that tells you not only if a point if properly classified or not, but also the magnitude of the distance in units of $\lVert w \rVert$.

##### **Geometric margins**

The vector $w$ is orthogonal to the separating hyperplane defined by $w^Tx + b = 0$. To see this, consider two points $x_1$ and $x_2$ on the hyperplane (see ESL Ch. 4.5):
$$
\begin{align}
w^Tx_1+b &= w^Tx_2+b = 0 & \text{by def. of the hyperplane} \\
w^Tx_1 &= w^Tx_2 \\
w^T(x_1 - x_2) & = 0 \\
\therefore \quad& w \perp \left\{x: w^Tx+b=0\right\}
\end{align}
$$
The **geometric margin** $\gamma^{(i)}$ is the distance from a training example $\left(x^{(i)}, y^{(i)} \right)$to the separating hyperplane. The projection of $x^{(i)}$ onto the separating hyperplane is the point $x^{(i)}-\gamma^{(i)} \frac{w}{\lVert w \rVert}$ (remember that $\gamma^{(i)} \in \mathbb{R}$ is just a scalar). Since this point is on the decision boundary, it satisfies $w^Tx+b=0$:

$$
\begin{align}
w^T \left(x^{(i)}-\gamma^{(i)} \frac{w}{\lVert w \rVert} \right) + b & = 0 \\
w^Tx^{(i)} - \gamma^{(i)} \frac{1}{\lVert w \rVert} w^T w + b & = 0 \hspace{7em} w^Tw = \lVert w \rVert^2\\
\gamma^{(i)} \lVert w \rVert & = w^Tx^{(i)} + b \\
\gamma^{(i)} & = \frac{w^Tx^{(i)} + b}{\lVert w \rVert} \\
\gamma^{(i)} & = \left(\frac{w}{\lVert w \rVert} \right)^T x^{(i)} + \frac{b}{\lVert w \rVert}
\end{align}
$$
To account for training examples on the other side of the decision boundary, we define the **geometric margin** of $\left(w,b \right)$ w.r.t. a training example $\left(x^{(i)}, y^{(i)} \right)$ to be:

$$
\gamma^{(i)} = y^{(i)} \left( \left(\frac{w}{\lVert w \rVert} \right)^T x^{(i)} + \frac{b}{\lVert w \rVert} \right)
$$

The geometric margin with $\lVert w \rVert = 1$ is equal to the functional margin.

The geometric margin is invariant to rescaling of the parameters $w$ and $b$. This means that we can impose an arbitrary scaling constraint on $w$, e.g. $\lVert w \rVert = 1$, $\lvert w_1 \rvert = 5$, or $\lvert w_1 + b \rvert + \lvert w_2\rvert = 2$, and any of these can be satisfied simply by rescaling $w$ and $b$.

Finally, given a training set $S = \left\{ \left(x^{(i)}, y^{(i)} \right); i = 1, \dots, m  \right\}$, we also define the geometric margin of $(w,b)$ w.r.t. $S$ to be the smallest of the geomtric margins on the individual training examples:

$$
\gamma = \min_{i=1, \dots, m} \gamma^{(i)}
$$

## **4. The optimal margin classifier**

Given a training set, a natural criterion is to set a decision boundary that maximizes the (geometric) margin, since this reflects a confident set of predictions on the training set.

Assume a training set that is linearly separable, i.e. a separating hyperplane can separate the two classes. Finding the hyperplane that maximizes the geometric margin is an optimization problem:

$$
\begin{align}
\max_{\gamma, w, b} \quad & \gamma \\
\text{s.t.} \quad & y^{(i)} \left(w^Tx^{(i)} + b \right) \geq \gamma, \quad i=1, \dots, m \\
                  & \lVert w \rVert = 1
\end{align}
$$

The above optimization problem is non-convex and therefore difficult to solve (due to the $\lVert w \rVert=1$ constraint). We can transform the problem, remembering that $\gamma$ is the geometric margin and $\hat{\gamma}$ is the functional margin:

$$
\begin{align}
\max_{\hat{\gamma}, w, b} \quad  & \frac{\hat{\gamma}}{\lVert w \rVert} \\
\text{s.t.} \quad & y^{(i)} \left(w^Tx^{(i)} + b \right) \geq \hat{\gamma}, \quad i=1, \dots, m
\end{align}
$$

This problem is equivalent since the geometric and functional margins are related by $\gamma = \frac{\hat{\gamma}}{\lVert w \rVert}$. This is better, since the $\lVert w \rVert=1$ constraint is gone, but the problem is still non-convex. To simplify it further, remember that $w$ and $b$ can be arbitrarily scaled without changing anything, and set the scaling by constraining the geometric margin of $w,b$ w.r.t. the training set to 1:

$$
\hat{\gamma} = 1
$$

$$
\begin{align}
\max_{\gamma, w, b} \quad  & \frac{1}{\lVert w \rVert} \\
= \min_{\gamma, w, b} \quad & \frac{1}{2} \lVert w \rVert^2  \\
\text{s.t.} \quad & y^{(i)} \left(w^Tx^{(i)} + b \right) \geq 1, \quad i=1, \dots, m
\end{align}
$$

This optimization problem has a convex quadratic objective and linear constraints. Solving it (using quadratic programming) gives the ***optimal margin classifier***. In order to solve this, we will use the method of Lagrange multipliers generalized to include inequality constraints in addition to equality constraints.

## **5. Digression: Lagrange duality**

Consider the following problem:

$$
\begin{align}
\min_w \quad      & f(w) \\
\text{s.t.} \quad & h_i(w) = 0, \quad i = 1, \dots, l
\end{align}
$$
This can be solved with [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier). Define the ***Lagrangian***:

$$
\mathcal{L} = f(w) + \sum_{i=1}^\ell \beta_i h_i (w)
$$
The $\beta_i$ are the ***Lagrange multipliers***.

To solve the problem, find the partial derivatives of $\mathcal{L}$, set to zero, and solve for $w$ and $\beta$:
$$
\frac{\partial \mathcal{L}}{\partial w_i} = 0; \frac{\partial \mathcal{L}}{\partial \beta_i} = 0.
$$
This can be generalized to constrained optimization problems with inequality as well as equality constraints. Consider the following, called the ***primal*** optimization problem:

$$
\begin{align}
\min_w \quad      & f(w) \\
\text{s.t.} \quad & g_i(w) \leq 0, \quad i=1, \dots, k \\
                  & h_i(w) = 0, \quad i=1, \dots, k
\end{align}
$$
To solve it, start by defining the ***generalized Lagrangian***

$$
\mathcal{L} \left(w, \alpha, \beta \right) = f(w) + \sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^\ell \beta_i h_i(w)
$$
Here, the $\alpha_i$'s and $\beta_i$'s are the Lagrange multipliers. Consider the quantity

$$
\theta_{\mathcal{P}}(w) = \max_{\alpha, \beta :  \alpha_i \geq 0} \mathcal{L} (w, \alpha, \beta),
$$

where the "$\mathcal{P}$" subscript stands for "primal". Let some $w$ be given. If $w$ violates any of the primal constraints (i.e., $g_i(w) > 0$ or $h_i(w) \neq 0$ for some $i$), then:

$$
\begin{align}
\theta_{\mathcal{P}}(w) & = \max_{\alpha, \beta: \alpha_i \geq 0} \left( f(w) + \sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^\ell \beta_i h_i(w) \right)\\
                        & = \infty
\end{align}
$$

Conversely, if the constraints are satisfied for a particular $w$, then $\theta_{\mathcal{P}}(w) = f(w)$. Hence,

$$
\theta_{\mathcal{P}}(w) = \begin{cases}
                          f(w) & \text{if } w \text{ satisfies primal constraints} \\
                          \infty  & \text{otherwise}
                          \end{cases}
$$

I.e., $\theta_{\mathcal{P}}$ takes the same value as the objective function for all values of $w$ that satisfy the primal constraints, and is positive infinity if the constraints are violated. Hence, the minimization problem

$$
\min_w \theta_{\mathcal{P}}(w) = \min_w \max_{\alpha, \beta: \alpha_i \geq 0} \mathcal{L} (w, \alpha, \beta)
$$

is the same problem (has the same solutions as) the original, primal problem. We also define the optimal value of the objective to be $p^* = \min_w \theta_{\mathcal{P}}(w)$; we call this the ***value*** of the primal problem.

Now, consider a slightly different problem. Define
$$
\theta_{\mathcal{D}}(\alpha, \beta) = \min_w \mathcal{L} (w, \alpha, \beta).
$$

Here, the "$\mathcal{D}$" subscript stands for "dual". Note that whereas in the definition of $\theta_{\mathcal{P}}$ we optimized w.r.t. $\alpha, \beta$, here we are minimizing w.r.t. $w$.

Now we define the ***dual*** optimization problem:
$$
\max_{\alpha, \beta: \alpha_i \geq 0} \theta_{\mathcal{D}}(\alpha, \beta) = \max_{\alpha, \beta: \alpha_i \geq 0} \min_w \mathcal{L} (w, \alpha, \beta).
$$

This is the same as the primal problem, except that the order of the max and min are exchanged. We also define the optimal value of the dual problem's objective to be $d^* = \max_{\alpha, \beta: \alpha \geq 0} \theta_{\mathcal{D}}(w)$.

The primal and dual problems are related in the following way:

$$
\begin{align}
d^* & \leq p^* \\
\max_{\alpha, \beta: \alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta) & \leq \min_w \max_{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta) \hspace{3em} \text{because } \max \min f \leq \min \max f
\end{align}
$$
Under certain conditions, $d^* = p^*$, so that the solution can be found by solving the dual problem instead of the primal problem. What are the conditions?

Suppose $f$ and the $g_i$'s are convex (for this purpose, this is when the Hessian is PSD), and the $h_i$'s are affine (i.e. linear, with an intercept term). Suppose further that the constraints $g_i$ are (strictly) feasible, i.e. there exists some $w$ s.t. $g_i(w) < 0 \hspace{.3em}\forall \hspace{.3em} i$.

Under these assumptions, there must exist $w^*, \alpha^*, \beta^*$ such that:

  - $w^*$ is the solution to the primal problem
  - $\alpha^*, \beta^*$ are the solution to the dual problem
  - $p^* = d^* = \mathcal{L}(w^*, \alpha^*, \beta^*)$
  
If some $w^*, \alpha^*, \beta^*$ satisfy the **Karush-Kuhn-Tucker (KKT) conditions**, then it is also a solution to the primal and dual problems:

$$
\begin{align}
\frac{\partial}{\partial w_i} \mathcal{L}(w^*, \alpha^*, \beta^*)     & = 0, \quad i=1, \dots, n \\
\frac{\partial}{\partial \beta_i} \mathcal{L}(w^*, \alpha^*, \beta^*) & = 0, \quad i=1, \dots, l \\
\alpha_i^* g_i (w^*)                                                  & = 0, \quad i=1, \dots, k & \text{the }\textbf{dual complementarity}\text{ condition}\\
g_i (w^*)                                                             & \leq 0, \quad i=1, \dots, k \\
\alpha^*                                                              & \geq 0, \quad i=1, \dots, k 
\end{align}
$$
The **dual complementarity** condition above implies:

  - if $\alpha_i^* > 0$, then $g_i(w^*)=0$
      - i.e, the $g_i(w) \leq 0$ constraint is **active** (holds with equality $g_i(w) = 0$ rather than w/inequality)
      - later, this is important for showing that the SVM has only a small number of support vectors
      
## **6. Optimal margin classifiers**

The primal optimization problem for finding the optimal margin classifier, derived in section 4:

$$
\begin{align}
\min_{\gamma, w, b} \quad & \frac{1}{2} \lVert w \rVert^2 \\
\text{s.t.}         \quad & y^{(i)} \left(w^Tx^{(i)} +b\right) \geq 1, \quad i=1, \dots, m
\end{align}
$$

The constraints can be written as:
$$
g_i(w) = -y^{(i} \left(w^Tx^{(i)} +b \right) + 1 \leq 0
$$
We have one constraint for each training example. From the KKT dual complementarity condition, $\alpha_i > 0$ only for the training examples that have geometric margin exactly equal to one (i.e., the ones corresponding to constraints that hold with equality $g_i(w)=0$). These training points are exactly the ones closest to the decision boundary, and are called the ***support vectors*** of the optimal margin classifier. The fact that the number of support vectors can be much smaller than the size of the training set will be useful later.

In writing the dual form of the problem, we will write in terms of inner products $\left\langle x^{(i)}, x^{(j)} \right\rangle$ between points in the input feature space. This will allow application of the kernel trick later.

The Lagrangian for the optimal margin classifier problem:
$$
\mathcal{L}(w, b, \alpha) = \frac{1}{2} \lVert w \rVert^2 - \sum_{i=1}^m \alpha_i\left[y^{(i)} \left(w^Tx^{(i)}+b \right) -1\right]
$$
Find the dual form $\theta_{\mathcal{D}} = \min_{w,b} \mathcal{L}(w, b, \alpha)$. Start by taking derivatives w.r.t. $w$ and $b$ and setting to zero:

$$
\begin{align}
\mathcal{L}(w, b, \alpha)           & = \frac{1}{2} \lVert w \rVert^2 - \sum_{i=1}^m \alpha_i \left[y^{(i)} \left(w^Tx^{(i)}+b \right) -1\right] \\
\nabla_w \mathcal{L}(w, b, \alpha)  & = w - \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)} = 0 \\
w & = \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}
\end{align}
$$

$$
\begin{align}
\mathcal{L}(w, b, \alpha)           & = \frac{1}{2} \lVert w \rVert^2 - \sum_{i=1}^m \alpha_i \left[y^{(i)} \left(w^Tx^{(i)}+b \right) -1\right] \\
\frac{\partial}{\partial b}\mathcal{L}(w, b, \alpha) & = - \sum_{i=1}^m \alpha_i y^{(i)} = 0 \\
\sum_{i=1}^m \alpha_i y^{(i)} & = 0
\end{align}
$$

$$
\begin{align}
\mathcal{L}(w, b, \alpha) & = \frac{1}{2} \lVert w \rVert^2 - \sum_{i=1}^m \alpha_i \left[y^{(i)} \left(w^Tx^{(i)}+b \right) -1\right] & \text{plug in }w \\
  & = \frac{1}{2} \sum_{i,j=1}^m  y^{(i)} y^{(j)} \alpha_i  \alpha_j \left(x^{(i)} \right)^Tx^{(j)} - \sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j \left(x^{(i)} \right)^T x^{(j)} - b \sum_{i=1}^m \alpha_i y^{(i)} + \sum_{i=1}^m \alpha_i & \sum_{i=1}^m \alpha_i y^{(i)} & = 0 \\
  & = \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j \left(x^{(i)} \right)^T x^{(j)}
\end{align}
$$

The dual optimization problem:
$$
\begin{align}
\max_{\alpha} W(\alpha) & = \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m y^{(i)} y^{(j)} \alpha_i \alpha_j \left(x^{(i)} \right)^T x^{(j)} \\
\text{s.t.} \hspace{.5em} \alpha_i & \geq 0, \quad i = 1, \dots, m \\
          & \sum_{i=1}^m \alpha_i y^{(i)} = 0 
\end{align}
$$
The conditions for $p^* = d^*$ and the KKT conditions to hold are satisfied in the above problem. If we solve this maximization problem w.r.t. the $\alpha_i$'s for the optimal $\alpha_i$'s, we can solve for the $w$ by the above equation ($w = \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)}$).

Then $b^*$ can be solved for (some kind of average?):

$$
b^* = - \frac{\max_{i:y^{(i)}=-1} w^{*T} x^{(i)} + \min_{i:y^{(i)}=1 } w^{*T}x^{(i)}}{2}
$$
Suppose the model has been fit to a training set. To make a prediction at a new input point $x$, calculate $w^Tx+b$, and predict $y=1$ iff this quantity is bigger than zero.

$$
\begin{align}
w^Tx + b  & = \left( \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)} \right)^Tx + b \\
          & = \sum_{i=1}^m \alpha_i y^{(i)} \langle x^{(i)}, x\rangle + b
\end{align}
$$
Hence, if we've found the $\alpha_i$'s, in order to make a prediction, we need to calculate a quantity that only depends on the inner product between $x$ and the points in the training set. Also, the $\alpha_i$'s will all be zero except for the support vectors. Thus, many of the terms in the sum will be zero, meaning that we only need to find the inner products between $x$ and the support vectors in order to make a prediction.

By using different kernels, the optimal margin classifier generalizes to the ***support vector machine***, which can learn efficiently in very high dimensional spaces.

## **Kernels**

  - def: original input values of problem = ***attributes*** (e.g. $x$, the living area of a house)
  - def: the inputs values that are passed to the learning algorithm = ***features*** (e.g. $\begin{bmatrix} x & x^2 & x^3\end{bmatrix}^T$)
  - def: the ***feature mapping*** $\phi$, a map from the attributes to the features, e.g.:
  $$
  \phi(x) = \begin{bmatrix}
            x   \\
            x^2 \\
            x^3
            \end{bmatrix}
  $$

Rather than applying an SVM using the original input attributes $x$, we may instead want to learn using some features $\phi(x)$. Since the SVM algorithm can be written entirely in terms of the inner products $\langle x,z \rangle$, this entails replacing all those inner products with $\langle \phi(x), \phi(z)\rangle$. Specifically, given a feature mapping $\phi$, we define the corresponding ***kernel*** to be 
$$
K(x,z) = \phi(x)^T \phi(z)
$$
Then, everywhere where the algorithm had $\langle x,z \rangle$, we replace it with $K(x,z)$ and the algorithm now learns with the features $\phi$.

Often, $K(x,z)$ is inexpensive to calculate, even though $\phi(x)$ itself may be very expensive (or impossible) to calculate (perhaps because it is an extremely high dimensional vector). In such settings, by using an efficient way to calculate $K(x,z)$ in the algorithm, an SVM can learn in the high dimensional feature space given by $\phi$, but without ever having to explicitly find or represent vectors $\phi(x)$.

An example: Suppose $x,z \in \mathbb{R}^n$, and consider
$$
K(x,z) = (x^Tz)^2
$$
This can also be written as
$$
\begin{align}
K(x,z)  & = \left(\sum_{i=1}^n x_i z_i \right) \left(\sum_{i=1}^n x_i z_i \right) \\
        & = \sum_{i=1}^n \sum_{j=1}^n x_i x_j z_i z_j \\
        & = \sum_{i,j=1}^n \left(x_i x_j \right) \left(z_i z_j \right) \\
        & = \phi(x)^T \phi(z), \hspace{2em} \text{where} \\
        & & \phi(x) = \begin{bmatrix}
                      x_1x_1 \\ x_1x_2 \\ x_1x_3 \\ x_2x_1 \\ x_2x_2 \\ x_2x_3 \\ x_3x_1 \\ x_3x_2 \\ x_3x_3
                      \end{bmatrix}\text{(for } n=3 \text{)} 
\end{align}
$$

Calculating the high-dimensional $\phi(x)$ requires $O(n^2)$ time, while calculating $K(x,z)$ requires only $O(n)$ time -- linear in the dimension of the input attributes.

Consider a related kernel, also consider

$$
\begin{align}
K(x,z)  & = \left(x^Tz + c \right)^2 \\
        & = \left(x^Tz \right)^2 + 2c\left(x^Tz \right) + c^2 \\
        & = \sum_{i,j=1}^n \left(x_i x_j \right)\left(z_i z_j \right) + \sum_{i=1}^n \left(\sqrt{2c} x_i \right) \left(\sqrt{2c} z_i \right) + c^2 \\
        & \hspace{5em} \phi(x) =  \begin{bmatrix}
                                  x_1x_1 \\ x_1x_2 \\ x_1x_2 \\ x_2x_1 \\ x_2x_2 \\ x_2x_3 \\ x_3x_3 \\ \sqrt{2c} x_1 \\ \sqrt{2c} x_2 \\ \sqrt{2c} x_3 \\ c
                                  \end{bmatrix} \\
\end{align}
$$

, where the parameter $c$ controls the relative weighting between the $x_i$ (first order) and the $x_i x_j$ (second order) terms.

More broadly, the kernel $K(x,z) = \left(x^Tz + c \right)^d$ corresponds to a feature mapping to an $\begin{pmatrix} n+d \\ d\end{pmatrix}$ feature space, corresponding of all monomials of the form $x_{i_2}x_{i_2} \dots x_{i_k}$ that are up to order $d$. Despite working in $O(n^d)$-dimensional space, computing $K(x,z)$ still takes only $O(n)$ time, and we never need to explicitly represent feature vectors in the very high dimensional feature space.

A slightly different view of kernels. Like the dot product, the kernel $K(x,z) = \phi(x)^T \phi(z)$ can be thought of as a similarity measure, i.e. if $\phi(x)$ and $\phi(z)$ are close together, we expect $K(x,z)$ to be large, and if $\phi(x)$ and $\phi(z)$ are far apart, we expect $K(x,z)$ to be small. Therefore, given a learning problem a good kernel to use would be a function represents the similarity of examples. For example, the function
$$
K(x,z) = \exp \left(- \frac{\lVert x-z \rVert^2}{2 \sigma^2} \right)
$$

might be a reasonable measure of the similarity of $x$ and $z$, as it is near $1$ when $x$ and $z$ are close, and near $0$ when $x$ and $z$ are far apart. However, is this a valid function to be used as a kernel? (In this case, yes, as this is the ***Gaussian kernel*** corresponding to an infinite-dimensional feature mapping $\phi$). But more broadly, given a function $K$, how can we tell if it's a valid kernel; i.e. is there a feature mapping $\phi$ such that $K(x,z) = \phi(x)^T \phi(z) \hspace{.2em} \forall x,z$?