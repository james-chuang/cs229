---
output:
  html_document:
    toc: TRUE
    fig_width: 4.5
    css: /home/ebosi/github/james-chuang.github.io/_sass/_style.scss
---

# **CS229 Autumn 2016 Problem Set #2**

### **1. Constructing kernels** 

In class, we saw that by choosing a kernel $K(x,z) = \phi(x)^T \phi(z)$, we can implicitly map data to a high dimensional space, and have the SVM algorithm work in that space. One way to generate kernels is to explicitly define the mapping $\phi$ to a higher dimensional space, and then work out the corresponding $K$.

However, in this question we are interested in direct construction of kernels, I.e., suppose we have a function $K(x,z)$ that we think gives an appropriate similarity measure for our learning problem, and we are considering plugging $K$ into the SVM as the kernel function. However, for $K(x,z)$ to be a valid kernel, it must correspond to an inner product in some higher dimensional space resulting from some feature mapping $\phi$. Mercer's theorem tells us that $K(x,z)$ is a (Mercer) kernel iff for any finite set $\left\{x^{(1)}, \dots, x^{(m)} \right\}$, the matrix $K$ is symmetric and positive semidefinite, where the square matrix $K \in \mathbb{R}^{m \times m}$ is given by $K_{ij} = K\left(x^{(i)}, x^{(j)} \right)$.

Now the question: Let $K_1, K_2$ be kernels over $\mathbb{R}^n \times \mathbb{R}^n$, let $a \in \mathbb{R}^+$ be a positive real number, let $f: \mathbb{R}^n \mapsto \mathbb{R}$, let $\phi: \mathbb{R}^n \rightarrow \mathbb{R}^d$, let $K_3$ be a kernel over $\mathbb{R}^d \times \mathbb{R}^d$, and let $p(x)$ be a polynomial over $x$ with *positive* coefficients.

For each of the functions $K$ below, state whether it is necessarily a kernel. If you think it is, prove it; if you think it isn't, give a counter-example.

[Hint: For part (e), the answer is that $K$ *is* indeed a kernel. You still have to prove it, though. (This one may be harder than the rest.) The result may also be useful for another part of the problem.]

**(a)** $K(x,z) = K_1(x,z) + K_2(x,z)$

The kernel matrix corresponding to $K(x,z)$ is symmetric since it is the sum of the two valid kernel matrices (i.e. symmetric matrices) $K_1$ and $K_2$. Therefore, $K(x,z)$ is PSD and therefore a valid kernel if $\forall \hspace{.2em}z, z^TKz \geq 0$:
$$
\begin{align}
& \qquad z^TKz \\
& = z^T \left(K_1 + K_2 \right)z \\
& = z^TK_1z + z^TK_2z && \text{matrix mult. distributive over addition} \\
& \geq 0  && z^TK_1z \geq 0, \quad z^TK_2z \geq 0 \text{ since they are valid kernels} \\
& \therefore K \text{ is a valid kernel}
\end{align}
$$


**(b)** $K(x,z) = K_1(x,z) - K_2(x,z)$:

Similar to 1a, $K(x,z)$ is symmetric since it is the difference of two symmetric matrices.
$$
\begin{align}
& \qquad z^TKz \\
& = z^T \left(K_1 - K_2 \right) z \\
& = z^T K_1z - z^TK_2z &\text{matrix mult. distributive over addition} \\
& \geq 0 \quad \text{only if } z^TK_1z > z^TK_2z \\
& \therefore K \text{ is not a valid kernel}
\end{align}
$$

**(c)** $K(x,z) = aK_1(x,z)$

$K(x,z)$ is symmetric because it is a symmetric matrix $K_1$ scaled by a scalar $a$.

$$
\begin{align}
& \qquad z^TKz \\
& = z^T\left(aK_1 \right)z \\
& = az^TK_1z \\
& \geq 0 & a \in \mathbb{R}^+,\quad z^TK_1z \geq 0 \hspace{.5em}\forall z \\
& \therefore K \text{ is a valid kernel}
\end{align}
$$

**(d)** $K(x,z) = -aK_1(x,z)$

By the same logic as 1c, $K$ is not a valid kernel.

**(e)** $K(x,z) = K_1(x,z)K_2(x,z)$



**(f)** $K(x,z) = f(x)f(z)$

$K(x,z)$ is symmetric since scalar multiplication is commutative.

$$
$$

**(g)** $K(x,z) = K_3(\phi(x), \phi(z))$

**(h)** $K(x,z) = p(K_1(x,z))$

### **2. Kernelizing the Perceptron**

**(a)**

**(b)**

**(c)**

### **3. Spam classification**

**(a)**

**(b)**

**(c)**

**(d)**

**(e)**

### **4. Properties of the VC dimension**

**(a)**

**(b)**

**(c)**

### **5. Training and testing on different distributions**

In the discussion in class about learning theory, a key assumption was that we trained and tested our learning algorithms on the same distribution $\mathcal{D}$. In this problem, we'll investigate one special case of training and testing on different distributions. Specifically, we will consider what happens when the training examples are *noisy*, but the test labels are not.

Consider a binary classification problem with labels $y \in \left\{0,1 \right\}$, and let $\mathcal{D}$ be a distribution over $(x,y)$, that we'll think of as the original, "clean" or "uncorrupted" distribution. Define $\mathcal{D}_{\mathcal{T}}$ to be a "corrupted" distribution over $(x,y)$ which is the same as $\mathcal{D}$, except that the labels $y$ have some probability $0 \leq \mathcal{T} < 0.5$ of being flipped. Thus, to sample from $\mathcal{D}_{\mathcal{T}}$, we would first sample $(x,y)$ from $\mathcal{D}$

**(a)**

**(b)**

**(c)**

### **6. Boosting and high energy physics**

**(a)**

**(b)**

**(c)**

**(d)**
